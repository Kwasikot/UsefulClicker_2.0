Ты работаешь в репозитории UsefulClicker, в котором уже реализован движок на Python (core/xml_engine.py),
LLM-клиенты (OpenAI, Ollama), voice-daemon и XML-сценарии. Теперь необходимо дополнить GUI-фронтенд для управления
кликером и доработать некоторые функции компьютерного зрения. 
The project consists of:
- core/xml_engine.py — XML execution engine. It parses XML scripts (<program>, <func>, <extnode>, <foreach>, etc.) and runs them.
- voice_daemon.py — background speech recognition service (Whisper). It can accumulate recognized phrases in _text_buffer and expose them via manual_flush().
- llm/openai_client.py — main LLM client (generate_text(prompt: str) -> str).
- llm/openai_client_compat.py — adapter that makes LLM usable from XML (<extnode>) by supporting multiple generate_text signatures.
- examples/*.xml — scenarios to run (pragmatism_youtube.xml, curiosity_drive_test_broad_topics.xml, etc.). They orchestrate voice input, LLM, and YouTube search actions.
- в каталоге ui/qt_ui. Там лежит файл mainwindow.ui. Это шаблон gui для qt5 приложения, которое является фронендом кликера.



Задачи:

1. Тестрирование функций xml узла PerceiveNode.

Описание PerceiveNodeTab (из mainwindow.ui).
Эта вкладка предназначена для тестирования компьютерного зрения в рамках проекта UsefulClicker.
В дальнейшем это будет отдельный xml узел PerceiveNode.
Пусть по кнопке perceiveButton кликер запускает процесс распознавания символов. 

Конкретная функция которая вызывает детекцию. 
В модуле cv/gui.py есть класс MainWindow, он распознает области с текстом на скриншоте.
Переименуй этот класс в PerceiveWindow.
Но для теста мне надо просто вызвать эту функцию и забрать массив прямоугольников.
Потом в каждом прямоугольнике надо вызвать ocr функцию из тех что я уже использовал.
Дополни код PerceiveWindow распознаванием текста в прямоугольниках через класс reader = easyocr.Reader(['ru', 'en']).

PerceiveWindow при создании вызовет функции распознавания прямоугольников и текстов в них.
После чего на виджете consoleText я должен увидеть отладочную печать массива прямоугольников с координатами и текстом внутри них.

2. Создание нового класса extnode ноды PerceiveNode.
И в xml графе это должна быть отдельная нода. PerceiveNode. 
<extnode module="perceive_node"
         class="PerceiveNode"
         method="perceive"
         output_var="rects_dict"
         output_format="dict"
/>


Этот текст идет пока во внутренний edit.
Потом мы этот массив прямоугольников и текстов дадим LLM

------------------------------------------------------------------------------------------------------
Произошла проблема с удалением файлов из git репозитория.
Файлы были удалены из репозитория начиная с Commit 0c11e5f.
Восстанови состояние репозитория до этого коммита.
------------------------------------------------------------------------------------------------------
По perceiveButton_2 делай следующее.
Посмотри дает ли библиотека easyocr информацию по bounding box текстовых строк.
Если да то сделай ocr всего скриншота, выведи текст и bounding boxes на consoleText.
------------------------------------------------------------------------------------------------------
[!ОТКАЗАЛСЯ ОТ ЭТОГО ПОДХОДА]
Напиши на Python модуль ocr_preproc.py, который выполняет предобработку результатов OCR (easyocr) перед вызовом LLM.

Задача

Из входных строк вида:

*d: Projects | UsefulClicker_2 Otperceive (conf=0.5865894764688891) bbox: ['(26,5)', '(285,5)', '(285,26)', '(26,26)']
node_promttxt (conf=0.925656959369695) bbox: ['(289,9)', '(395,9)', '(395,25)', '(289,25)']
Notepad+ + (conf=0.9482106320445134) bbox: ['(405,7)', '(485,7)', '(485,25)', '(405,25)']
Eile (conf=0.967438614596726) bbox: ['(7,33)', '(35,33)', '(35,51)', '(7,51)']
Edit (conf=0.9999983310699463) bbox: ['(47,33)', '(79,33)', '(79,53)', '(47,53)']
Search (conf=0.9999990753271193) bbox: ['(93,35)', '(141,35)', '(141,53)', '(93,53)']


получить:

список токенов (слова/фрагменты) с нормализованным текстом, conf и bbox,

список строк (группировка токенов по геометрии) с агрегированным bbox и средней уверенностью,

топ-K кандидатов по намерению пользователя (intent) с локальным скорингом,

компактный LLM-промт для rerank/выбора по кандидатам.

Требования к модулю

Сделай чистый, самодостаточный код без внешних сервисов. Разрешены re, json, argparse, math, dataclasses, typing, collections, itertools. (Опционально rapidfuzz — если нет, сделай свою простую метрику похожую на ratio).

Функции

Реализуй следующие функции с типами:

from typing import List, Dict, Tuple, Any

def normalize_text(s: str) -> str:
    """Нормализация OCR-текста: lower, trim, фиксы частых OCR-ошибок ('|'->'l', '+ +'->'++', '—'->'-', множественные пробелы). Возвращает нормализованную строку."""

def parse_bbox(points: List[str]) -> Tuple[int, int, int, int]:
    """На вход список строк вида ['(x1,y1)','(x2,y2)','(x3,y3)','(x4,y4)'].
    Вернуть (xmin, ymin, xmax, ymax) как int."""

def parse_ocr_lines(lines: List[str]) -> List[Dict[str, Any]]:
    """Парсит строки easyocr. Возвращает список токенов:
    [{'id': int, 'raw_text': str, 'text': str, 'conf': float, 'bbox': [x1,y1,x2,y2],
      'cx': float, 'cy': float, 'w': int, 'h': int}]"""

def group_into_lines(tokens: List[Dict[str, Any]],
                     y_overlap_ratio: float = 0.6,
                     y_merge_tol_px: int = 4) -> List[Dict[str, Any]]:
    """Группирует токены в строки по вертикальному перекрытию и близости Y.
    Сортирует строки по порядку чтения (y, затем x).
    Возвращает [{'line_id': int, 'text': str, 'raw_texts': List[str], 'token_ids': List[int],
                 'avg_conf': float, 'bbox':[x1,y1,x2,y2], 'cx': float, 'cy': float}]"""

def fuzzy_ratio(a: str, b: str) -> float:
    """Простая метрика похожая на fuzzy WRatio (0..100).
    Если rapidfuzz доступен — используй его, иначе сделай нормализованную Levenshtein-приблизительную оценку (или token-sort ratio)."""

def score_line(line: Dict[str, Any], intent: str) -> float:
    """Вернуть численный скор: комбинируй fuzzy (0.8), avg_conf (0.1), и размер bbox (0.1 с лог-усилением)."""

def rank_candidates(lines: List[Dict[str, Any]], intent: str, top_k: int = 8) -> List[Dict[str, Any]]:
    """Отсортировать строки по score и вернуть top_k. Добавь поле 'score'."""

def build_llm_prompt(candidates: List[Dict[str, Any]], intent: str) -> str:
    """Собрать компактный системный промт для LLM-rerank (не включай координаты):
    - краткая инструкция
    - intent
    - список кандидатов: [ID=<line_id>] text="..." context_hint="..." 
    (context_hint получи как raw_texts объединённые или соседние строки, на твой выбор)
    - формат ответа: 'Return the ID only or NONE'."""

Поведение/алгоритмы

Парсинг входа: регулярками вытащи raw_text, conf=(...), bbox: ['(x,y)', ...]. Регистр и пробелы могут «плавать».

Геометрия: для каждого токена вычисли (cx, cy, w, h).

Группировка в строку: два токена в одной строке, если вертикальное перекрытие ≥ y_overlap_ratio * min(h_i, h_j) и |y_centers| ≤ y_merge_tol_px. Строка получает bbox как min/max всех токенов, text — конкатенация нормализованных text с одиночным пробелом; avg_conf — среднее.

Порядок чтения: сперва по ymin (с допуском — сгладь кластеры близких y), потом по xmin.

Скоринг:

fuzzy = fuzzy_ratio(line["text"], intent)            # 0..100
conf  = line["avg_conf"] * 100                       # 0..100
area  = (x2-x1)*(y2-y1)
area_boost = min(30.0, (max(area,1)**0.5)/10.0)      # 0..~30
score = 0.8*fuzzy + 0.1*conf + 0.1*area_boost


Игнорируй строки с пустым текстом, очень маленькой площадью (например, area < 15) или avg_conf < 0.25.

LLM-промт: включи только top-K. Для каждого кандидата добавь line_id, text (без координат), короткий context_hint (например, первые 1–2 соседние строки или исходные фрагменты этой строки). В конце чётко укажи формат ответа: «Верни только ID (число) лучшего совпадения; если ни один кандидат не подходит — верни 'NONE'.»

CLI

Сделай запуск как скрипт:

python ocr_preproc.py --input ocr.txt --intent "find search menu" --topk 8 --output out.json


--input — путь к файлу со строками OCR (по одной строке).

--intent — строка намерения пользователя.

--topk — число кандидатов (по умолчанию 8).

--output — путь для JSON-выхода.

Выводи JSON со структурой:

{
  "tokens": [...],
  "lines": [...],
  "candidates": [...], 
  "llm_prompt": "..."
}

Тест-кейс (включи в блок if __name__ == "__main__": как опцию --demo)

При --demo используй встроённые sample-строки (см. вверху), intent "open search menu", выведи результат в stdout без файла.

Качество

Строгая типизация, dataclasses приветствуются.

Докстринги и комментарии к ключевой логике.

Отсутствие глобальных побочных эффектов.

Устойчивость к кривым OCR-строкам: пропускай непарсибельные элементы с предупреждением на stderr.

Сгенерируй полностью готовый файл ocr_preproc.py.

По perceiveButton_2 делай следующее.
Сделай ocr всего скриншота, выведи текст и bounding boxes на consoleText.
Запусти предобработку используя ocr_preproc.py и build_llm_prompt.
Выведи результирующий промт на consoleText.

------------------------------------------------------
Подай на вход функции build_llm_prompt строку intent из виджета userIntent.
Подай промт на llm используя интерфейсы openai_client или ollama_client.
В этом тесте пока захардкодь  provider="ollama" model="llama3.2:latest"
------------------------------------------------------\
Доработай функцию def group_into_lines чтобы она группировала строки файлового меню в одну строку
------------------------------------------------------\

------------------------------------------------------------

PROMPT FOR CODEX_CLI (RUS):

Напиши полностью готовый Python-скрипт ready_layout_infer.py, который использует готовые предобученные модели для анализа макета скриншота и группировки OCR-токенов в строки без обучения.

Что должен уметь скрипт

Загрузить изображение скриншота.

Выделить блоки (layout regions) с помощью одной из готовых моделей:

Вариант A (Hugging Face): попытаться загрузить microsoft/dit-base-finetuned-doclaynet через AutoImageProcessor + AutoModelForObjectDetection.

Фоллбэк B (layoutparser + Detectron2): модель PubLayNet (lp.Detectron2LayoutModel), например конфиг lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config.

Если обе недоступны, аккуратно логировать предупреждение и перейти к простому CV-базлайну (бинаризация + поиск крупных текстовых блоков контурным методом).

Принять OCR-токены от EasyOCR:

либо из файла --ocr-json (формат: {"tokens":[{"id":..,"text":"...","conf":..,"bbox":[x1,y1,x2,y2]}, ...]}),

либо, если задан --run-ocr, выполнить EasyOCR внутри скрипта (язык en, можно параметризовать).

Привязать токены к блокам: токен относится к блоку, если центр bbox попал внутрь bbox блока (с допуском по краям --assign-pad пикселей).

Склеить токены в строки внутри каждого блока геометрическим способом:

группировка по вертикальному перекрытию и близости базовой линии (как в ранее описанной функции group_into_lines),

спец-обработка верхней полосы (менюбар): если токен попал в полосу [0, menubar_top + menubar_height_ratio*median_h], слить всё в одну строку.

Отсортировать строки по порядку чтения (ymin, затем xmin).

Сохранить результат в JSON:

{
  "image_size":[W,H],
  "blocks":[{"block_id":1,"bbox":[x1,y1,x2,y2],"type":"text","score":0.92}],
  "lines":[
    {"line_id":1,"block_id":1,"token_ids":[...],"text":"file edit search","bbox":[...],"avg_conf":0.98}
  ]
}


(Опционально) Сохранить overlay-превью (--preview out.png) с рамками блоков и строк.

Зависимости

Обязательные: opencv-python, numpy, Pillow, argparse, json, typing, math.

Опциональные:

Hugging Face: transformers, torch (для варианта A).

layoutparser + Detectron2 (для варианта B).

easyocr (если --run-ocr).

Скрипт не должен падать, если опциональных пакетов нет: корректно логируй и используй фоллбэк.

CLI
python ready_layout_infer.py \
  --image screen.png \
  --ocr-json ocr.json \
  --out result.json \
  --preview overlay.png \
  --menubar-top 0 \
  --menubar-height-ratio 1.5 \
  --assign-pad 4 \
  --cv-baseline 1 \
  [--run-ocr] [--ocr-lang en] [--min-token-area 12]


Аргументы:

--image — путь к изображению (обяз.).

--ocr-json — путь к JSON с токенами; если не задан и указан --run-ocr, извлечь токены через EasyOCR.

--run-ocr — включить EasyOCR внутри скрипта.

--ocr-lang — языки EasyOCR (деф. en).

--menubar-top и --menubar-height-ratio — для слияния верхней полосы (меню) в одну строку.

--assign-pad — допуск по краям при отнесении токена к блоку.

--cv-baseline — разрешить fallback-CV блок-детекцию.

--out — JSON-выход; если не задан, печатать в stdout.

--preview — PNG с наложенными блоками и строками.

--min-token-area — фильтр мелких токенов.

------------------------------------------------------------------------------------
Выпили старую логику: нам не нужны более функции 
normalize_text,
parse_bbox,
parse_ocr_lines,
group_into_lines,
fuzzy_ratio,
score_line,
rank_candidates,
build_llm_prompt,

На предыдущем этапе ты получил независимый модуль ready_layout_infer.py теперь это надо интегрировать в qt-ui клиент.
Ты получаешь layout_result.json. 
Теперь составь промт к llm используя строки "lines" из этого json обьекта для матчинга строки по интенту пользователя.
Строку intent из виджета userIntent.
Подай промт на llm используя интерфейсы openai_client или ollama_client.
В этом тесте пока захардкодь  provider="ollama" model="llama3.2:latest"
Вывод с llm как и раньше в окно consoleText.
------------------------------------------------------------------------------------
